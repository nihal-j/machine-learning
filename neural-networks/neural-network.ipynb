{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.set_printoptions(precision=4, linewidth=100, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, method='min-max'):\n",
    "    \n",
    "    '''\n",
    "        Normalize `data` using `method`. Computes statistics assuming each sample is a row.\n",
    "\n",
    "        Arguments:\n",
    "            data:       data to be normalized\n",
    "            method:     if 'min-max' then normalization is used, else standardization is used\n",
    "\n",
    "        Returns:\n",
    "            Normalized data   \n",
    "    '''\n",
    "    \n",
    "    if (method == 'min-max'):\n",
    "        numerator = data - np.min(data, axis=0)\n",
    "        denominator = np.max(data, axis=0) - np.min(data, axis=0)\n",
    "        return numerator/denominator\n",
    "    \n",
    "    if (method == 'standardization'):\n",
    "        numerator = data - np.mean(data, axis=0)\n",
    "        denominator = np.std(data, axis=0)\n",
    "        return numerator/denominator\n",
    "    \n",
    "def segregate_target(data):\n",
    "\n",
    "    '''\n",
    "        Segregates `data` into (X, t) tuple where `X` has each example as a column and `t`\n",
    "        is the corresponding class label.\n",
    "    '''\n",
    "    \n",
    "    X = data[:, :-1]\n",
    "    t = data[:, -1:]\n",
    "    \n",
    "    return X, t\n",
    "\n",
    "def train_test_validation_split(X, t, test_ratio=0.33):\n",
    "\n",
    "    '''\n",
    "        Make use of sklearn's `train_test_split` to split `X` into train, test and validation sets.\n",
    "    '''\n",
    "    \n",
    "    X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=test_ratio, random_state=42)\n",
    "    X_valid, X_test, t_valid, t_test = train_test_split(X_test, t_test, test_size=0.5, random_state=42)\n",
    "    \n",
    "    data = {\n",
    "        'X_train': X_train,\n",
    "        't_train': t_train,\n",
    "        'X_valid': X_valid,\n",
    "        't_valid': t_valid,\n",
    "        'X_test': X_test,\n",
    "        't_test': t_test\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_data(path):\n",
    "\n",
    "    '''\n",
    "        Load .npy data specified at `path`.\n",
    "    '''\n",
    "    \n",
    "    data = np.load(path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, L_, n_, w_={}, b_={}, activation_='relu', learning_rate_=0.0001, max_iters_=1000):\n",
    "        \n",
    "        \"\"\"\n",
    "            A Model is a deep neural network whose configuration is contained in `n`. Its paramters are\n",
    "            defined in `w` and biases in `b`. The activation functions for hidden layers can be specified as\n",
    "            'relu' or 'sigmoid'. The output layer always uses 'sigmoid' activation.\n",
    "            \n",
    "            Arguments:\n",
    "                L_:             number of layers in neural network (excluding input layer)\n",
    "                n_:             neural network configuration, specifically `n_[i]` is the number of nodes in layer i\n",
    "                w_:             weight parameters for each layer, specifically `w_[i]` is the weight matrix of layer i\n",
    "                b_:             biases for each layer, specifically `b_[i]` is the bias vector for layer i\n",
    "                activation_:    activation for each hidden layer\n",
    "                learning_rate_: learning rate for updation of weights\n",
    "                max_iters_:     maximum number of iterations during training\n",
    "        \"\"\"\n",
    "        \n",
    "        self.L = L_\n",
    "        self.n = n_\n",
    "        self.w = w_\n",
    "        self.b = b_\n",
    "        self.activation = activation_\n",
    "        self.learning_rate = learning_rate_\n",
    "        self.max_iters = max_iters_\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "    \n",
    "        # the ith hidden layer should be of shape (n[i - 1], n[i])\n",
    "        for i in range(1, self.L + 1):\n",
    "            self.w[i] = np.random.randn(self.n[i - 1], self.n[i])\n",
    "            self.b[i] = np.random.randn(self.n[i], 1)\n",
    "            \n",
    "            \n",
    "    def sigmoid(self, a):\n",
    "\n",
    "        x = np.copy(a)\n",
    "        x[x < -15] = -15\n",
    "        x[x > 15] = 15\n",
    "\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "\n",
    "    def relu(self, a):\n",
    "\n",
    "        x = np.copy(a)\n",
    "        x[x < 0] = 0\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def activation_function(self, a):\n",
    "\n",
    "        if activation == 'relu':\n",
    "            return self.relu(a)\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            return self.sigmoid(a)\n",
    "        \n",
    "\n",
    "    def derivative(self, a):\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            x = np.copy(a)\n",
    "            x[x > 0] = 1\n",
    "            x[x < 0] = 0\n",
    "            return x\n",
    "\n",
    "        if self.activation == 'sigmoid':\n",
    "            return self.sigmoid(a)*self.sigmoid(1 - a)\n",
    "        \n",
    "    \n",
    "    def forward_propagate(self, X):\n",
    "\n",
    "        z = {}\n",
    "        a = {}\n",
    "        z[0] = X\n",
    "        M = X.shape[1]\n",
    "        for i in range(1, self.L):\n",
    "            a[i] = np.matmul(self.w[i].T, z[i - 1]).reshape(self.n[i], M) + self.b[i]\n",
    "            z[i] = self.activation_function(a[i])\n",
    "        a[self.L] = np.dot(self.w[self.L].T, z[self.L - 1]) + self.b[self.L]\n",
    "        z[self.L] = self.sigmoid(a[self.L])\n",
    "\n",
    "        return a, z\n",
    "    \n",
    "    \n",
    "    def back_propagate(self, a, z, t):\n",
    "\n",
    "        delta = {}\n",
    "        dw = {}\n",
    "        db = {}\n",
    "\n",
    "        delta[self.L] = z[self.L] - t\n",
    "        dw[self.L] = np.matmul(z[self.L - 1], delta[self.L].T)\n",
    "        db[self.L] = np.sum(delta[self.L], axis=1, keepdims=True)\n",
    "\n",
    "        for i in range(self.L - 1, 0, -1):\n",
    "\n",
    "            delta[i] = self.derivative(a[i])*np.matmul(self.w[i + 1], delta[i + 1])\n",
    "            dw[i] = np.matmul(z[i - 1], delta[i].T)\n",
    "            db[i] = np.sum(delta[i], axis=1, keepdims=True)\n",
    "\n",
    "        return dw, db\n",
    "    \n",
    "    \n",
    "    def update(self, dw, db):\n",
    "\n",
    "        for i in range(1, self.L + 1):\n",
    "            \n",
    "            self.w[i] = self.w[i] - eta*dw[i]\n",
    "            self.b[i] = self.b[i] - eta*db[i]\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "    \n",
    "        a, z = self.forward_propagate(X)\n",
    "        preds = np.copy(z[L])\n",
    "        preds[preds > 0.5] = 1\n",
    "        preds[preds < 0.5] = 0\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, X, t):\n",
    "        \n",
    "        for i in range(self.max_iters):\n",
    "            \n",
    "            a, z = self.forward_propagate(X)\n",
    "            dw, db = self.back_propagate(a, z, t)\n",
    "            self.update(dw, db)\n",
    "            \n",
    "        preds = self.predict(X)\n",
    "        training_accuracy = calculate_accuracy(preds, t)\n",
    "        \n",
    "        return training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('data/data.npy')\n",
    "data = normalize(data)\n",
    "X = data[0]\n",
    "t = X[-1]\n",
    "X = X[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some global parameters\n",
    "\n",
    "# number of hidden layers\n",
    "L = 0\n",
    "# number of nodes in each layer; n[i] is number of nodes in ith layer (numbering starts from 0)\n",
    "n = []\n",
    "# parameters w\n",
    "w = {}\n",
    "# parameters b\n",
    "b = {}\n",
    "# choice of activation function\n",
    "activation = 'relu'\n",
    "# learning rate\n",
    "eta = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Models(L_, n_, eta_, activation_='relu'):\n",
    "    \n",
    "    global L, n, w, b, activation\n",
    "    \n",
    "    L = L_\n",
    "    n = n_\n",
    "    activation = activation_\n",
    "    eta = eta_\n",
    "    w = {}\n",
    "    b = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \n",
    "    global L, n, w, b, activation\n",
    "    \n",
    "    # the ith hidden layer should be of shape (n[i - 1], n[i])\n",
    "    for i in range(1, L + 1):\n",
    "        w[i] = np.random.randn(n[i - 1], n[i])\n",
    "        b[i] = np.random.randn(n[i], 1)\n",
    "        # w[i] = np.ones((n[i - 1], n[i]))\n",
    "        # b[i] = np.ones((n[i], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "\n",
    "    x = np.copy(a)\n",
    "    x[x < -15] = -15\n",
    "    x[x > 15] = 15\n",
    "    \n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def relu(a):\n",
    "    \n",
    "    x = np.copy(a)\n",
    "    x[x < 0] = 0\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def activation_function(a):\n",
    "    \n",
    "    global activation\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        return relu(a)\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        return sigmoid(a)\n",
    "    \n",
    "def derivative(a):\n",
    "    \n",
    "    global activation\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        x = np.copy(a)\n",
    "        x[x > 0] = 1\n",
    "        x[x < 0] = 0\n",
    "        return x\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        return sigmoid(a)*sigmoid(1 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X):\n",
    "    \n",
    "    global L, n, w, b, activation\n",
    "    \n",
    "    z = {}\n",
    "    a = {}\n",
    "    z[0] = X\n",
    "    M = X.shape[1]\n",
    "    for i in range(1, L):\n",
    "        a[i] = np.matmul(w[i].T, z[i - 1]).reshape(n[i], M) + b[i]\n",
    "        z[i] = activation_function(a[i])\n",
    "    a[L] = np.dot(w[L].T, z[L - 1]) + b[L]\n",
    "    z[L] = sigmoid(a[L])\n",
    "    \n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(a, z, t):\n",
    "    \n",
    "    global L, n, w, b, activation\n",
    "    \n",
    "    delta = {}\n",
    "    dw = {}\n",
    "    db = {}\n",
    "    \n",
    "    # print(z[L].shape, t.shape)\n",
    "    \n",
    "    delta[L] = z[L] - t\n",
    "    #print('delta[',L,']:', '\\n',delta[L])\n",
    "    dw[L] = np.matmul(z[L - 1], delta[L].T)\n",
    "    db[L] = np.sum(delta[L], axis=1, keepdims=True)\n",
    "    #print('dw[',L,']:', '\\n',dw[L])\n",
    "    #print('db[',L,']:', '\\n',db[L])\n",
    "    \n",
    "    for i in range(L - 1, 0, -1):\n",
    "        \n",
    "        delta[i] = derivative(a[i])*np.matmul(w[i + 1], delta[i + 1])\n",
    "        #print('derivative: ', derivative(a[i]))\n",
    "        #print('other term: ', np.matmul(w[i + 1], delta[i + 1]))\n",
    "        #print('delta[',i,']:', '\\n',delta[i])\n",
    "        dw[i] = np.matmul(z[i - 1], delta[i].T)\n",
    "        #print('dw[',i,']:', '\\n',dw[i])\n",
    "        db[i] = np.sum(delta[i], axis=1, keepdims=True)\n",
    "        #print('db[',i,']:', '\\n',db[i])\n",
    "        \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(dw, db):\n",
    "    \n",
    "    global w, b, eta\n",
    "    \n",
    "    for i in range(1, L + 1):\n",
    "        w[i] = w[i] - eta*dw[i]\n",
    "        b[i] = b[i] - eta*db[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    \n",
    "    a, z = forward_propagate(X)\n",
    "    preds = np.copy(z[L])\n",
    "    preds[preds > 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def calculate_accuracy(y, t):\n",
    "    \n",
    "    return 100 - (np.mean(np.abs(y - t))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.13698630136986\n"
     ]
    }
   ],
   "source": [
    "L_ = 2\n",
    "n_ = [10, 20, 1]\n",
    "activation_ = 'relu'\n",
    "eta_ = 0.001\n",
    "Models(L_, n_, eta_, activation_)\n",
    "initialize_parameters()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "#pp.pprint(w)\n",
    "#pp.pprint(b)\n",
    "\n",
    "data = load_data('data/data.npy')\n",
    "t = data[:,-1]\n",
    "X = data[:,:-1]\n",
    "X = normalize(X)\n",
    "X = X.T\n",
    "t = t.reshape(1, -1)\n",
    "# X = np.array([[1, 2], [0, 1]])\n",
    "# t = np.array([[1, 0]])\n",
    "# print(X.shape, t.shape)\n",
    "\n",
    "for i in range(5000):\n",
    "    #print('Iteration :', i)\n",
    "    a,z = forward_propagate(X)\n",
    "    #pp.pprint(a)\n",
    "    #pp.pprint(z)\n",
    "    dw, db = back_propagate(a, z, t)\n",
    "    update(dw, db)\n",
    "    #pp.pprint('\\n')\n",
    "\n",
    "preds = predict(X)\n",
    "print(calculate_accuracy(preds, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0334]\n",
      " [0.6667]\n",
      " [0.5   ]\n",
      " [0.1401]\n",
      " [0.6667]\n",
      " [0.5   ]\n",
      " [0.375 ]\n",
      " [0.5   ]\n",
      " [0.    ]\n",
      " [0.3865]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:,0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_ = 2\n",
    "n_ = [10, 20, 1]\n",
    "activation_ = 'relu'\n",
    "eta_ = 0.01\n",
    "model = Model(L_, n_, activation_='relu', learning_rate_=eta, max_iters_=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('data/data.npy')\n",
    "t = data[:,-1]\n",
    "X = data[:,:-1]\n",
    "X = normalize(X)\n",
    "X = X.T\n",
    "t = t.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.68493150684931"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
